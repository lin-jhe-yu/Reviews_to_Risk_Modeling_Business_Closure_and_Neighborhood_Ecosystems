{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619421a5-94a5-4581-806a-40e0477a7bd8",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9019077a-9728-4a82-afaf-dc3d02a36d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 17:31:32 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+------------+--------------------+--------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+---------------+-----------------+--------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+-------------+---------+--------------------+------------------------+\n",
      "|             address|avg_rating|            category|         description|             gmap_id|               hours|          latitude|   longitude|          store_name|num_of_reviews|price|    relative_results|               state|  MISC_Accessibility|MISC_Activities|      MISC_Amenities|MISC_Atmosphere|       MISC_Crowd| MISC_Dining_options|MISC_From_the_business|MISC_Getting_here|MISC_Health_and_safety|MISC_Highlights|MISC_Lodging_options|      MISC_Offerings|       MISC_Payments|       MISC_Planning|    MISC_Popular_for|MISC_Recycling|MISC_Service_options|  zip|     zip_city|zip_state|          zip_county|irs_estimated_population|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+------------+--------------------+--------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+---------------+-----------------+--------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+-------------+---------+--------------------+------------------------+\n",
      "|Noodles and Compa...|       4.0|[Restaurant, Down...|Counter-serve cha...|0x80dc0b0fed60f38...|                null|33.081466999999996|-117.2359845| Noodles and Company|             8|   $$|[0x80dc0b0fed60f3...|                null|                null|           null|                null|           null|             null|                null|                  null|             null|                  null|           null|                null|                null|                null|                null|                null|          null|          [Delivery]|92009|     Carlsbad|       CA|    San Diego County|                   41410|\n",
      "|Marhaba halal res...|       4.5| [Indian restaurant]|Casual neighborho...|0x808581f568090ac...|[[Monday, 11:30AM...|        37.7798273|-122.4321928|Marhaba halal res...|            68|   $$|[0x808580a6c524f1...|Open ⋅ Closes 11:...|[Wheelchair acces...|           null|[Good for kids, H...|       [Casual]|[Family-friendly]|[Lunch, Dinner, C...|                  null|             null|                  null|           null|                null|[Comfort food, Ha...|[Debit cards, Cre...|[Accepts reservat...|[Lunch, Dinner, S...|          null|[No-contact deliv...|94115|San Francisco|       CA|San Francisco County|                   25250|\n",
      "|Water Store & Ras...|       4.3|        [Restaurant]|                null|0x80c36737b882df6...|[[Saturday, 8AM–4...|        34.4494749|-117.2773874|Water Store & Ras...|             8| null|[0x80c36737c72555...|  Permanently closed|                null|           null|     [Good for kids]|           null|             null|                null|                  null|             null|                  null|           null|                null|                null|                null|                null|                null|          null|          [Delivery]|92345|     Hesperia|       CA|San Bernardino Co...|                   77040|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+------------+--------------------+--------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+---------------+-----------------+--------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+--------------------+-----+-------------+---------+--------------------+------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/cleaned_restaurant_meta_df/\")\n",
    "meta_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b083390f-4bbe-47a2-8c54-a56ac9bb855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171061\n"
     ]
    }
   ],
   "source": [
    "print(meta_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a23fdff-fb11-4efe-9000-d41f2ec60b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------+----+--------------------+-------------+--------------------+\n",
      "|             gmap_id|       cust_name|rating|resp|                text|         time|             user_id|\n",
      "+--------------------+----------------+------+----+--------------------+-------------+--------------------+\n",
      "|0x8085bc9abc80160...|Justin Kuzmanich|     5|null|The fried chicken...|1621568098557|11696804019802722...|\n",
      "|0x80c2b8ad72ddf62...|  Oscar Figueroa|     5|null|Really good, good...|1481836410347|10826010437179732...|\n",
      "|0x809009d22f6669c...|Alberto Gallardo|     4|null|                null|1603813476043|10639545859216517...|\n",
      "+--------------------+----------------+------+----+--------------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "review_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/cleaned_restaurant_review_df/\")\n",
    "review_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5711049-ff2a-4704-b307-e9cbfa7eed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51585575"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140d3c1e-e170-4c41-bcf9-9be506d5eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+------+----+--------------------+-------------+--------------------+--------------------+----------+--------------------+-----------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+------------------+------------------+---------------+---------------+---------------+--------------------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+-------------+-------------+--------------------+--------------+--------------------+-----+--------+---------+-------------+------------------------+\n",
      "|             gmap_id|     cust_name|rating|resp|                text|         time|             user_id|             address|avg_rating|            category|description|               hours|          latitude|         longitude|          store_name|num_of_reviews|price|    relative_results|             state|MISC_Accessibility|MISC_Activities| MISC_Amenities|MISC_Atmosphere|          MISC_Crowd|MISC_Dining_options|MISC_From_the_business|MISC_Getting_here|MISC_Health_and_safety|MISC_Highlights|MISC_Lodging_options|      MISC_Offerings|MISC_Payments|MISC_Planning|    MISC_Popular_for|MISC_Recycling|MISC_Service_options|  zip|zip_city|zip_state|   zip_county|irs_estimated_population|\n",
      "+--------------------+--------------+------+----+--------------------+-------------+--------------------+--------------------+----------+--------------------+-----------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+------------------+------------------+---------------+---------------+---------------+--------------------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+-------------+-------------+--------------------+--------------+--------------------+-----+--------+---------+-------------+------------------------+\n",
      "|0x4065fd476208a27...|Chiderah Abani|     5|null|Love, love, love....|1528890236792|11208554345809883...|New York Kim-Bob ...|       4.3|[Korean restauran...|       null|[[Monday, 7AM–9PM...|40.764643199999995|-73.81192399999999|New York Kim-Bob ...|            68|    $|[0x89c2602e5c9513...|Closed ⋅ Opens 7AM|              null|           null|[Good for kids]| [Casual, Cozy]|[College students...|               null|                  null|             null|                  null|           null|                null|[Comfort food, He...|         null|         null|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|11354|Flushing|       NY|Queens County|                   51190|\n",
      "|0x4065fd476208a27...|  luis cabrera|     5|null|My favorite Kimba...|1589581105858|11034451447099652...|New York Kim-Bob ...|       4.3|[Korean restauran...|       null|[[Monday, 7AM–9PM...|40.764643199999995|-73.81192399999999|New York Kim-Bob ...|            68|    $|[0x89c2602e5c9513...|Closed ⋅ Opens 7AM|              null|           null|[Good for kids]| [Casual, Cozy]|[College students...|               null|                  null|             null|                  null|           null|                null|[Comfort food, He...|         null|         null|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|11354|Flushing|       NY|Queens County|                   51190|\n",
      "|0x4065fd476208a27...|     SUNG CHOI|     5|null|                null|1485139260721|10431700601085267...|New York Kim-Bob ...|       4.3|[Korean restauran...|       null|[[Monday, 7AM–9PM...|40.764643199999995|-73.81192399999999|New York Kim-Bob ...|            68|    $|[0x89c2602e5c9513...|Closed ⋅ Opens 7AM|              null|           null|[Good for kids]| [Casual, Cozy]|[College students...|               null|                  null|             null|                  null|           null|                null|[Comfort food, He...|         null|         null|[Lunch, Dinner, S...|          null|[Delivery, Takeou...|11354|Flushing|       NY|Queens County|                   51190|\n",
      "+--------------------+--------------+------+----+--------------------+-------------+--------------------+--------------------+----------+--------------------+-----------+--------------------+------------------+------------------+--------------------+--------------+-----+--------------------+------------------+------------------+---------------+---------------+---------------+--------------------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------------+-------------+-------------+--------------------+--------------+--------------------+-----+--------+---------+-------------+------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "combined_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/combined_rest_df/\") \n",
    "combined_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7512c74-7c82-49b7-bcb7-df7dc0a93491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Love, love, love. If you are looking for authentic, great-tasting Korean dishes at reasonable prices, this is the place to go.\n",
      "\n",
      "I travel all the way from upstate NY (about 2hrs) just to eat here.    |\n",
      "|My favorite Kimbap spot in queens ! Absolutely delicious, the owner is very kind to all the customers. I can truly taste the love in the recipe. My go to is the spicy tuna & beef Kimbap!! :)         |\n",
      "|null                                                                                                                                                                                                   |\n",
      "|(Translated by Google) It was delicious and friendly, the surroundings were very clean, the price was cheap and good.\n",
      "\n",
      "(Original)\n",
      "맛있고 친절하고 주변 환경도 매우 깨끗하고 가격도 저렴하고 좋았습니다.|\n",
      "|(Translated by Google) It can be submitted and even a light meal... good\n",
      "\n",
      "(Original)\n",
      "투고도 되고 간단한 식사까지...good                                                                                |\n",
      "|(Translated by Google) Good place for a quick meal\n",
      "\n",
      "(Original)\n",
      "간단한 식사 하기 괜찮은곳                                                                                                               |\n",
      "|Wonderful atmosphere and great food.                                                                                                                                                                   |\n",
      "|null                                                                                                                                                                                                   |\n",
      "|null                                                                                                                                                                                                   |\n",
      "|Great food and vibe!                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.select(\"text\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c2aa793-1544-4122-893e-02259d000c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51585575"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e55f0b4a-a0f7-4dba-8dc8-9ffe51edd485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28263274"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "combined_df.filter(\n",
    "    (col(\"text\").isNotNull()) & \n",
    "    (col(\"rating\").isNotNull())\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09c8e92a-e67f-4586-b023-e93e82893bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "combined_df = combined_df.withColumn(\n",
    "    \"irs_estimated_population\",\n",
    "    col(\"irs_estimated_population\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Add permanent closed flag\n",
    "combined_df = combined_df.withColumn(\n",
    "    \"permanent_closed\",\n",
    "    when(col(\"state\").like(\"%Permanently closed%\"), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02513355-e6ea-483b-a7d1-3ccfd5a59bc8",
   "metadata": {},
   "source": [
    "## NLP Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74e12808-fc0e-4541-9a23-3e06b83e3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf14629-2382-4528-b09b-be5f1c72fd8c",
   "metadata": {},
   "source": [
    "#### Use TF-IDF to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ad53f21-cfec-4c5a-8dc0-b1bab411f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                 |rating|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|(5000,[17,1276,1390,2276,2550,3681,3745,4232,4391,4424],[7.4872221681858795,5.57799550929756,2.094104331819578,3.529381896821388,4.487392949000265,6.712891206782917,4.7130911822999195,4.774571940068023,4.076055487238508,6.007732825861522])                                                                                          |5     |\n",
      "|(5000,[113,421,1044,1053,1086,1240,1451,2301,3112,3300,3618,3690,3739,4890],[4.50783573586615,6.312115233746733,4.411649316903328,4.555397864487655,3.891743204567587,2.680079618813369,2.881105673315482,4.729879662794569,5.907046758813355,4.282366994318205,7.735014911066389,5.427771643933317,4.927182110734094,4.136778602906843])|5     |\n",
      "|(5000,[1726,1738,2353,4465],[3.0517215200301395,4.138248172773618,5.849181883858978,5.040471933593748])                                                                                                                                                                                                                                  |5     |\n",
      "|(5000,[1124,1168,1406,1720],[8.916952518610842,1.566221830969205,3.4448293026522547,6.169326481707476])                                                                                                                                                                                                                                  |5     |\n",
      "|(5000,[1168,1390,1915,4868],[1.566221830969205,2.094104331819578,4.020078017238673,4.043523157462896])                                                                                                                                                                                                                                   |4     |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Keep only rows where both text and rating are not null\n",
    "# ----------------------------\n",
    "combined_df_text = combined_df.filter(\n",
    "    (col(\"text\").isNotNull()) & \n",
    "    (col(\"rating\").isNotNull())\n",
    ")\n",
    "# ----------------------------\n",
    "# 2. Tokenize\n",
    "# ----------------------------\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized_df = tokenizer.transform(combined_df_text)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Remove stop words\n",
    "# ----------------------------\n",
    "stop_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
    "clean_df = stop_remover.transform(tokenized_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Keep only English tokens\n",
    "# ----------------------------\n",
    "english_udf = udf(lambda tokens: [t.lower() for t in tokens if re.fullmatch(r\"[a-zA-Z]+\", t)],\n",
    "                  ArrayType(StringType()))\n",
    "\n",
    "english_df = clean_df.withColumn(\"english_tokens\", english_udf(col(\"tokens_clean\")))\n",
    "\n",
    "# ----------------------------\n",
    "# 5. TF-IDF features\n",
    "# ----------------------------\n",
    "hashingTF = HashingTF(inputCol=\"english_tokens\", outputCol=\"tf_features\", numFeatures=5000)\n",
    "tf_df = hashingTF.transform(english_df)\n",
    "\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(tf_df)\n",
    "tfidf_df = idf_model.transform(tf_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Assemble final features (TF-IDF only)\n",
    "# ----------------------------\n",
    "assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
    "feature_df = assembler.transform(tfidf_df)\n",
    "\n",
    "# Now feature_df is ready for ML models (e.g., GBTClassifier)\n",
    "feature_df.select(\"features\", \"rating\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4f333c8-14d9-4951-8205-b994a9ec19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:============================================>           (35 + 9) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28263274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(feature_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8af0764f-3d18-4b9f-86f3-13e529bb3c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:======================================================> (43 + 1) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------+----+----+-------+-------+----------+--------+-----------+------+--------+---------+----------+--------------+-------+----------------+--------+------------------+---------------+--------------+---------------+----------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------+-------------+-------------+----------------+--------------+--------------------+---+--------+---------+----------+------------------------+----------------+------+------------+--------------+-----------+--------------+--------+\n",
      "|gmap_id|cust_name|rating|    resp|text|time|user_id|address|avg_rating|category|description| hours|latitude|longitude|store_name|num_of_reviews|  price|relative_results|   state|MISC_Accessibility|MISC_Activities|MISC_Amenities|MISC_Atmosphere|MISC_Crowd|MISC_Dining_options|MISC_From_the_business|MISC_Getting_here|MISC_Health_and_safety|MISC_Highlights|MISC_Lodging_options|MISC_Offerings|MISC_Payments|MISC_Planning|MISC_Popular_for|MISC_Recycling|MISC_Service_options|zip|zip_city|zip_state|zip_county|irs_estimated_population|permanent_closed|tokens|tokens_clean|english_tokens|tf_features|tfidf_features|features|\n",
      "+-------+---------+------+--------+----+----+-------+-------+----------+--------+-----------+------+--------+---------+----------+--------------+-------+----------------+--------+------------------+---------------+--------------+---------------+----------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------+-------------+-------------+----------------+--------------+--------------------+---+--------+---------+----------+------------------------+----------------+------+------------+--------------+-----------+--------------+--------+\n",
      "|      0|        0|     0|25590040|   0|   0|      0|      0|         0|       0|    6084054|801746|       0|        0|         0|             0|2991194|          552513|13214506|           2247368|       28263274|       2726435|        4029930|   4172656|            4560256|              27124748|         28263274|              27669048|        9456211|            28263274|       2288252|      8773503|     15521406|         4424198|      28263215|              786260|  0|       0|        0|         0|                       0|               0|     0|           0|             0|          0|             0|       0|\n",
      "+-------+---------+------+--------+----+----+-------+-------+----------+--------+-----------+------+--------+---------+----------+--------------+-------+----------------+--------+------------------+---------------+--------------+---------------+----------+-------------------+----------------------+-----------------+----------------------+---------------+--------------------+--------------+-------------+-------------+----------------+--------------+--------------------+---+--------+---------+----------+------------------------+----------------+------+------------+--------------+-----------+--------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "feature_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in feature_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "633860ba-8c27-4917-b9ef-3876d3adaa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+----------+\n",
      "|cust_name                |rating|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |store_name           |avg_rating|\n",
      "+-------------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+----------+\n",
      "|everyday dahye           |1     |(Translated by Google) I'm going to leave many Koreans anyway. Today I ate rabokki, gimbap and oden soup and ate it in the hall. I've never seen a staff member in front of me saying that I want to buy more water. It wasn't our table, but it was bad. Because we had to start noticing the server as well. When we ordered, we sat close to the counter and we got an order from the checkout. I leave 1 star on the first cold. The food tasted good, but I won't go back.\n",
      "\n",
      "(Original)\n",
      "어차피 한국사람들 많이가니까 그냥 한국말로 남길게요. 오늘 여기서 라볶이, 김밥, 오뎅국 시켜서 홀에서 먹었는데요. 저는 살다살다 물 더 가져다 달란다고 앞에서 대놓고 아이씨 하는 직원은 처음 봤네요. 저희 테이블도 아니였지만 덩달아 기분나빴습니다. 왜냐하면 저희도 서버분 눈치를 보기 시작해야 했거든요 저희가 주문할때도 계산대와 가까이 앉아있다고 주문을 계산대에서 받는데 귀찮은 티도 나고 짜증내는게 눈에 보여 그 뒤로부턴 저희가 서버분 눈치봤습니다 기분좋게 밥먹으러 왔다가 세상 처음 받아보는 냉대에 별점 1개 남기고 갑니다. 음식맛은 괜찮았지만 다신 안갈 것 같아요|New York Kim-Bob NaRa|4.3       |\n",
      "|Hee Ran Huh              |2     |Stopped by to law office today. Was hungry so stopped by New York Kim bob.\n",
      "Ordered spicy pork over rice, kimbap, spicy rice cake and stir fry udon with octopus.\n",
      "Spicy rice cake was too salty, stir fry udon didn't like the taste of it.\n",
      "It's a long wait!. Waited nearly 25 min for my food to come out.\n",
      "I was looking for something quick and leave.\n",
      "Didn't like the waitinf time.\n",
      "Only one person working in the register. NOT good.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |New York Kim-Bob NaRa|4.3       |\n",
      "|Jessica Strull-Diagostino|2     |Donuts are rarely fresh and the staff is slow and the orders are barely correct. Buts it’s cheap.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Dunkin'              |3.9       |\n",
      "|Robert Haverstock        |2     |They messed up my order and then on free donut day couldn't even give us the donuts for the four drinks that was ordered...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Dunkin'              |3.9       |\n",
      "|Kerri Goble              |1     |I would give 0 stars if able, don’t give them your business.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Dunkin'              |3.9       |\n",
      "+-------------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df.filter(col(\"rating\") <= 2) \\\n",
    "    .select(\"cust_name\", \"rating\", \"text\", \"store_name\", \"avg_rating\") \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09014db3-8ce6-4d78-b3b0-0c6d87939795",
   "metadata": {},
   "source": [
    "#### Evaluate model performance for Linear Regression, GBTRegressor, and RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0097a045-55e4-4f7e-ac40-ebf6b3e9f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "def train_evaluate_regression(df, features_col, target_col=\"sentiment_score\",\n",
    "                              model_class=None, model_params=None,\n",
    "                              sample_fraction=None, train_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model on a PySpark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame with features and target column\n",
    "        features_col: str, column name of assembled features vector\n",
    "        target_col: str, target column name\n",
    "        model_class: PySpark ML regression class, e.g., GBTRegressor\n",
    "        model_params: dict, optional parameters for the model\n",
    "        sample_fraction: float, fraction to sample for prototyping\n",
    "        train_ratio: float, train/test split ratio\n",
    "        seed: int, random seed\n",
    "    Returns:\n",
    "        model: fitted PySpark ML model\n",
    "        predictions: DataFrame with predictions\n",
    "        metrics: dict, contains RMSE, MAE, R2\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rename target column if needed\n",
    "    df = df.withColumnRenamed(target_col, \"label\")\n",
    "    \n",
    "    # Optional sampling\n",
    "    if sample_fraction:\n",
    "        df = df.sample(fraction=sample_fraction, seed=seed)\n",
    "    \n",
    "    # Train/test split\n",
    "    train_df, test_df = df.randomSplit([train_ratio, 1-train_ratio], seed=seed)\n",
    "    \n",
    "    # Initialize model\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "    model = model_class(featuresCol=features_col, labelCol=\"label\", **model_params)\n",
    "    \n",
    "    # Build pipeline\n",
    "    pipeline = Pipeline(stages=[model])\n",
    "    \n",
    "    # Fit model\n",
    "    fitted_model = pipeline.fit(train_df)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = fitted_model.transform(test_df)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    metrics = {}\n",
    "    for metric_name in [\"rmse\", \"mae\", \"r2\"]:\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=metric_name)\n",
    "        metrics[metric_name] = evaluator.evaluate(predictions)\n",
    "        print(f\"{metric_name.upper()}: {metrics[metric_name]}\")\n",
    "    \n",
    "    return fitted_model, predictions, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fc0e5-f9e8-4454-8e95-7f76ab78fbdb",
   "metadata": {},
   "source": [
    "#### Linear Regression (TF-IDF) - 1% sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4cbebdb-1a08-44ca-900b-945766d61791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9885875464880052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7519309183288498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 778:=====================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.39881555328707685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor, RandomForestRegressor, LinearRegression\n",
    "\n",
    "# Linear Regression\n",
    "lr_model, lr_predictions, lr_metrics = train_evaluate_regression(\n",
    "    df=feature_df,\n",
    "    features_col=\"features\",\n",
    "    target_col=\"rating\",\n",
    "    model_class=LinearRegression,\n",
    "    model_params={\"maxIter\": 20, \"regParam\": 0.1, \"elasticNetParam\": 0.0}, \n",
    "    sample_fraction=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12beec-730f-4026-a779-63c7a916eda3",
   "metadata": {},
   "source": [
    "#### GBTRegressor (TF-IDF) - 1% sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b135236-6aa0-44f9-aaa5-6c24ef7f350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0341088645191803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7826221413212312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 988:=====================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.34217558545885685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# GBTRegressor\n",
    "gbt_model, gbt_preds, gbt_rmse = train_evaluate_regression(\n",
    "    df=feature_df,\n",
    "    features_col=\"features\",\n",
    "    target_col=\"rating\",\n",
    "    model_class=GBTRegressor,\n",
    "    model_params={\"maxIter\": 20, \"maxDepth\": 5},\n",
    "    sample_fraction=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2ef4e-e097-4af3-8df5-9d361cd796cf",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor (TF-IDF) - 1% sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b73b433-47d6-4d35-8764-242d79741767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 18:44:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1064.3 KiB\n",
      "25/11/29 18:46:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1406.4 KiB\n",
      "25/11/29 18:46:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/11/29 18:46:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/11/29 18:46:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1310340316954544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.892369962943114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:======================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.20599477368261554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# RandomForestRegressor\n",
    "rf_model, rf_preds, rf_rmse = train_evaluate_regression(\n",
    "    df=feature_df,\n",
    "    features_col=\"features\",\n",
    "    target_col=\"rating\",\n",
    "    model_class=RandomForestRegressor,\n",
    "    model_params={\"numTrees\": 50, \"maxDepth\": 5},\n",
    "    sample_fraction=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39f753-d080-4673-ab5f-ac4cb19bd8db",
   "metadata": {},
   "source": [
    "#### Evaluate transformer based (bert) model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2c97321-15aa-4a25-86cf-5abbfac72692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L4_512 download started this may take some time.\n",
      "Approximate size to download 104 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql.functions import col\n",
    "from sparknlp.base import DocumentAssembler, EmbeddingsFinisher\n",
    "from sparknlp.annotator import Tokenizer, BertEmbeddings, SentenceEmbeddings\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Filter usable rows\n",
    "# -----------------------------\n",
    "df = combined_df.filter(\n",
    "    col(\"text\").isNotNull() & (col(\"text\") != \"\") & col(\"rating\").isNotNull()\n",
    ").withColumnRenamed(\"rating\", \"sentiment_score\")\n",
    "\n",
    "# Optional sampling to reduce compute\n",
    "df = df.sample(0.01, seed=42)\n",
    "\n",
    "# Split train/test\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Spark NLP pipeline (compute embeddings)\n",
    "# -----------------------------\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings.pretrained(\"small_bert_L4_512\", \"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"bert_token_embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"bert_token_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "embeddings_finisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCols([\"finished_sentence_embeddings\"]) \\\n",
    "    .setOutputAsVector(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "flatten_vector = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "        SELECT *, finished_sentence_embeddings[0] AS features\n",
    "        FROM __THIS__\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(predictions):\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=\"sentiment_score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=\"sentiment_score\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=\"sentiment_score\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "    \n",
    "    return rmse, mae, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bee77-4d22-499e-9458-5ee5c03c77b1",
   "metadata": {},
   "source": [
    "#### bert based LinearRegression - 1% sample size¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fc7d33a-3c61-4d4c-ba84-1d6a21df3618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 17:23:13 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "25/11/29 17:23:13 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|text                                                                                                                                                                                                                                                                                                                                                                                     |sentiment_score|prediction        |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|Food is delicious and reasonably priced. Only negative is that it often makes me sick, just the type of food, not quality of restaurant 😊                                                                                                                                                                                                                                               |4              |2.9883802683101393|\n",
      "|Friendly employees , beautiful view,                                                                                                                                                                                                                                                                                                                                                     |5              |4.483447074792082 |\n",
      "|Clean and well organized. Very pleasant barista!!                                                                                                                                                                                                                                                                                                                                        |5              |4.752764582178459 |\n",
      "|Slow service but the food was Taco Bell standards                                                                                                                                                                                                                                                                                                                                        |3              |3.860618338337278 |\n",
      "|The kimchee burger is bomb!                                                                                                                                                                                                                                                                                                                                                              |5              |4.030596350703456 |\n",
      "|Best Shrimp chipotle taco I have EVER had. The tortilla was thin like it was handmade and toasted to perfection, not too brittle not too soft. Plenty of shrimp and topped with all of the right fixins including a generous helping of cotija cheese.\n",
      "The young man who took our order was courteous, helpful and respectful. Not something you find much these days.\n",
      "Highly recommended|5              |3.970136274911672 |\n",
      "|Always been my favorite fast food                                                                                                                                                                                                                                                                                                                                                        |5              |4.466019055471781 |\n",
      "|Atmosphere was excellent, outside eating was very enjoyable, excellent service with good chips and salsa, an excellent food with humongous portions. We will definitely be back and recommend it to anyone.                                                                                                                                                                              |5              |4.933119512891212 |\n",
      "|Amazing food! I also keep coming back because the owners are so wonderful. I get fast super friendly service and they're even willing to substitute items and accommodate different diets. They also have the best salsa!                                                                                                                                                                |5              |5.01104847730059  |\n",
      "|The food here is amazingly good tasting and fresh! The menu is extensive. You can definitely find what you are looking for.                                                                                                                                                                                                                                                              |5              |5.44327123920211  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 672:====================================================>  (49 + 2) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LINEAR REGRESSION =====\n",
      "RMSE: 0.8923304438333948\n",
      "MAE:  0.6770822652935277\n",
      "R²:   0.5107742936763873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Build pipeline for Linear Regression\n",
    "lr = LinearRegression(labelCol=\"sentiment_score\", featuresCol=\"features\", maxIter=20, regParam=0.1, elasticNetParam=0.0)\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    bert_embeddings,\n",
    "    sentence_embeddings,\n",
    "    embeddings_finisher,\n",
    "    flatten_vector,\n",
    "    lr\n",
    "])\n",
    "\n",
    "# Fit and predict\n",
    "model_lr = pipeline_lr.fit(train_df)\n",
    "predictions_lr = model_lr.transform(test_df)\n",
    "\n",
    "predictions_lr.select(\"text\", \"sentiment_score\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "rmse, mae, r2 = evaluate_model(predictions_lr)\n",
    "print(\"\\n===== LINEAR REGRESSION =====\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE:  {mae}\")\n",
    "print(f\"R²:   {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109bf04-31c0-4b78-a93d-3b0ef47cc7e1",
   "metadata": {},
   "source": [
    "#### bert based GBTRegressor - 1% sample size¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687fc56-f07a-4254-8bd0-df8de22a5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|text                                                                                                                                                                                                       |sentiment_score|prediction        |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|Food is delicious and reasonably priced. Only negative is that it often makes me sick, just the type of food, not quality of restaurant 😊                                                                 |4              |3.4696206851636844|\n",
      "|Same as always                                                                                                                                                                                             |5              |4.242307624868072 |\n",
      "|Good place to go to, fully staffed crew during lunch rush so waits aren't too bad.                                                                                                                         |4              |3.9891335495704854|\n",
      "|Amazing food, great selection of beer and wine, and an awesome staff!!!                                                                                                                                    |5              |4.953412635255394 |\n",
      "|Trash                                                                                                                                                                                                      |2              |2.8323852185190623|\n",
      "|Waited in front for 20 minutes and when they finally came out she said the forgot me. WTF                                                                                                                  |1              |2.922215547294044 |\n",
      "|Atmosphere was excellent, outside eating was very enjoyable, excellent service with good chips and salsa, an excellent food with humongous portions. We will definitely be back and recommend it to anyone.|5              |4.7542658371659545|\n",
      "|Goodbjjk NJ j                                                                                                                                                                                              |5              |4.690743846652877 |\n",
      "|Good food.  Great service                                                                                                                                                                                  |5              |4.85460285074404  |\n",
      "|Very peaceful place to come and relax                                                                                                                                                                      |4              |4.675824389325839 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 660:=====================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GBT REGRESSOR =====\n",
      "RMSE: 0.9409336066032937\n",
      "MAE:  0.6695532348359932\n",
      "R²:   0.45537769200975287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Build pipeline for GBT\n",
    "gbt = GBTRegressor(labelCol=\"sentiment_score\", featuresCol=\"features\", maxIter=20, maxDepth=5)\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    bert_embeddings,\n",
    "    sentence_embeddings,\n",
    "    embeddings_finisher,\n",
    "    flatten_vector,\n",
    "    gbt\n",
    "])\n",
    "\n",
    "# Fit and predict\n",
    "model_gbt = pipeline_gbt.fit(train_df)\n",
    "predictions_gbt = model_gbt.transform(test_df)\n",
    "\n",
    "predictions_gbt.select(\"text\", \"sentiment_score\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "rmse, mae, r2 = evaluate_model(predictions_gbt)\n",
    "print(\"\\n===== GBT REGRESSOR =====\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE:  {mae}\")\n",
    "print(f\"R²:   {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c4842-a362-4492-ab38-1051b8d40f3f",
   "metadata": {},
   "source": [
    "#### bert based RandomForestRegressor - 1% sample size¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d6318-186a-4e87-a21b-c2bcb0426dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 17:51:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1200.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|text                                                                                                                                                                                                       |sentiment_score|prediction        |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|Food is delicious and reasonably priced. Only negative is that it often makes me sick, just the type of food, not quality of restaurant 😊                                                                 |4              |3.4584546209893032|\n",
      "|Same as always                                                                                                                                                                                             |5              |3.9230829045745725|\n",
      "|Good place to go to, fully staffed crew during lunch rush so waits aren't too bad.                                                                                                                         |4              |3.8146012536265443|\n",
      "|Amazing food, great selection of beer and wine, and an awesome staff!!!                                                                                                                                    |5              |4.784328535393297 |\n",
      "|Trash                                                                                                                                                                                                      |2              |3.6292404365999134|\n",
      "|Waited in front for 20 minutes and when they finally came out she said the forgot me. WTF                                                                                                                  |1              |2.7278779248249134|\n",
      "|Atmosphere was excellent, outside eating was very enjoyable, excellent service with good chips and salsa, an excellent food with humongous portions. We will definitely be back and recommend it to anyone.|5              |4.516081374781259 |\n",
      "|Goodbjjk NJ j                                                                                                                                                                                              |5              |4.467600070929743 |\n",
      "|Good food.  Great service                                                                                                                                                                                  |5              |4.77057414172083  |\n",
      "|Very peaceful place to come and relax                                                                                                                                                                      |4              |4.637167703214584 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 693:===================================================>   (48 + 3) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RANDOM FOREST REGRESSOR =====\n",
      "RMSE: 0.9996217764256213\n",
      "MAE:  0.7660091844406082\n",
      "R²:   0.3853202825496663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Build pipeline for Random Forest\n",
    "rf = RandomForestRegressor(labelCol=\"sentiment_score\", featuresCol=\"features\", numTrees=50, maxDepth=5)\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    bert_embeddings,\n",
    "    sentence_embeddings,\n",
    "    embeddings_finisher,\n",
    "    flatten_vector,\n",
    "    rf\n",
    "])\n",
    "\n",
    "# Fit and predict\n",
    "model_rf = pipeline_rf.fit(train_df)\n",
    "predictions_rf = model_rf.transform(test_df)\n",
    "\n",
    "predictions_rf.select(\"text\", \"sentiment_score\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "rmse, mae, r2 = evaluate_model(predictions_rf)\n",
    "print(\"\\n===== RANDOM FOREST REGRESSOR =====\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE:  {mae}\")\n",
    "print(f\"R²:   {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc86cdf-1241-4230-984c-cf948019ae41",
   "metadata": {},
   "source": [
    "#### Train bert-based linear regression and lable whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6199ded0-a0a4-4cad-b584-e06e18098b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# If you haven't already, create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Set Spark log level to WARN or ERROR (suppress INFO and repeated WARN)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Optionally, suppress some specific loggers\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "log4jLogger.LogManager.getLogger(\"org.apache.spark.scheduler.DAGScheduler\").setLevel(log4jLogger.Level.ERROR)\n",
    "log4jLogger.LogManager.getLogger(\"org.apache.spark.storage.BlockManager\").setLevel(log4jLogger.Level.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941e8a81-425c-49f4-8421-f88692320df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql.functions import col\n",
    "from sparknlp.base import DocumentAssembler, EmbeddingsFinisher\n",
    "from sparknlp.annotator import Tokenizer, BertEmbeddings, SentenceEmbeddings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75fe9031-2c4e-48b6-bd89-4da51ccadd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 1. Filter usable rows\n",
    "# --------------------------------------------------\n",
    "df = combined_df.filter(\n",
    "    col(\"text\").isNotNull() &\n",
    "    (col(\"text\") != \"\") &\n",
    "    col(\"rating\").isNotNull()\n",
    ").withColumnRenamed(\"rating\", \"sentiment_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccced66-35d8-4e46-8576-69119256a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L4_512 download started this may take some time.\n",
      "Approximate size to download 104 MB\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=>(43 + 1) / 44][Stage 45:=>(43 + 1) / 44][Stage 171:(31 + 54) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L4_512 download started this may take some time.\n",
      "Approximate size to download 104 MB\n",
      "Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=>(43 + 1) / 44][Stage 45:=>(43 + 1) / 44][Stage 171:(32 + 53) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=>(43 + 1) / 44][Stage 45:=>(43 + 1) / 44][Stage 171:(33 + 54) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=>(43 + 1) / 44][Stage 45:=>(43 + 1) / 44][Stage 171:(36 + 54) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "=== Fitting embedding pipeline (this runs BERT once) ===\n",
      "=== Transforming sample to embeddings ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=>(43 + 1) / 44][Stage 45:=>(43 + 1) / 44][Stage 171:(37 + 54) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running CV (much faster now) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: gs://msca-bdp-student-gcs/Group_5_final_project/cv_best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CV RESULTS (BERT + Linear Regression) =====\n",
      "RMSE: 0.8896228800214185\n",
      "MAE:  0.6733536627395328\n",
      "R²:   0.5137400785605513\n"
     ]
    }
   ],
   "source": [
    "# Sample 1%\n",
    "sample_frac = 0.01\n",
    "df_sample = df.sample(fraction=sample_frac, seed=42)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. PRECOMPUTE EMBEDDINGS (done ONCE)\n",
    "# --------------------------------------------------\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings.pretrained(\"small_bert_L4_512\", \"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"bert_token_embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"bert_token_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "embeddings_finisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCols([\"finished_sentence_embeddings\"]) \\\n",
    "    .setOutputAsVector(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "flatten_vector = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "        SELECT *, finished_sentence_embeddings[0] AS features\n",
    "        FROM __THIS__\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "embedding_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    bert_embeddings,\n",
    "    sentence_embeddings,\n",
    "    embeddings_finisher,\n",
    "    flatten_vector\n",
    "])\n",
    "\n",
    "print(\"=== Fitting embedding pipeline (this runs BERT once) ===\")\n",
    "embedding_model = embedding_pipeline.fit(df_sample)\n",
    "\n",
    "print(\"=== Transforming sample to embeddings ===\")\n",
    "df_emb = embedding_model.transform(df_sample).cache()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Train/test split **after** embeddings\n",
    "# --------------------------------------------------\n",
    "train_df, test_df = df_emb.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Linear Regression + Cross Validation (FAST)\n",
    "# --------------------------------------------------\n",
    "lr = LinearRegression(\n",
    "    labelCol=\"sentiment_score\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=20\n",
    ")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"sentiment_score\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=8   # uses 8 of your 16 total cores\n",
    ")\n",
    "\n",
    "print(\"=== Running CV (much faster now) ===\")\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# SAVE BEST MODEL (INSERT HERE)\n",
    "# --------------------------------------------------\n",
    "best_model_path = \"gs://msca-bdp-student-gcs/Group_5_final_project/cv_best_model\"\n",
    "cv_model.bestModel.write().overwrite().save(best_model_path)\n",
    "print(\"Best model saved to:\", best_model_path)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Evaluate\n",
    "# --------------------------------------------------\n",
    "predictions = cv_model.transform(test_df)\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "mae = RegressionEvaluator(labelCol=\"sentiment_score\", predictionCol=\"prediction\",\n",
    "                          metricName=\"mae\").evaluate(predictions)\n",
    "r2 = RegressionEvaluator(labelCol=\"sentiment_score\", predictionCol=\"prediction\",\n",
    "                         metricName=\"r2\").evaluate(predictions)\n",
    "\n",
    "print(\"\\n===== CV RESULTS (BERT + Linear Regression) =====\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE:  {mae}\")\n",
    "print(f\"R²:   {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60af315-44df-4b5d-938a-ad20a7cdfd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted embedding pipeline to disk (or GCS)\n",
    "embedding_model_path = \"gs://msca-bdp-student-gcs/Group_5_final_project/embedding_pipeline_model\"\n",
    "embedding_model.write().overwrite().save(embedding_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd54d8-0106-4f65-b03d-c9d30ebc4a3d",
   "metadata": {},
   "source": [
    "#### decide the sampling strategy from distriubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f3801b-3d31-4e7f-a999-2089498d0450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+----+----+------+-----+-----+\n",
      "|zip_state|avg_reviews_per_store| q10| q25|median|  q75|  q90|\n",
      "+---------+---------------------+----+----+------+-----+-----+\n",
      "|       CA|    172.4717666316166|12.0|36.0|  97.0|213.0|384.0|\n",
      "|       IL|    180.4268143967109|13.0|38.0| 105.0|227.0|400.0|\n",
      "|       NY|   143.81782464017962|10.0|28.0|  74.0|168.0|320.0|\n",
      "+---------+---------------------+----+----+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Count number of reviews per store\n",
    "df_reviews_per_store = df.groupBy(\"gmap_id\", \"zip_state\") \\\n",
    "    .agg(F.count(\"*\").alias(\"review_count\"))\n",
    "\n",
    "# 2) Compute average number of reviews per store, grouped by zip_state\n",
    "df_avg_reviews = df_reviews_per_store.groupBy(\"zip_state\") \\\n",
    "    .agg(F.avg(\"review_count\").alias(\"avg_reviews_per_store\"))\n",
    "\n",
    "# 3) Compute quantiles per zip_state\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]  # 10th, 25th, median, 75th, 90th percentiles\n",
    "zip_states = [row['zip_state'] for row in df_reviews_per_store.select('zip_state').distinct().collect()]\n",
    "\n",
    "results = []\n",
    "for zs in zip_states:\n",
    "    counts = df_reviews_per_store.filter(F.col(\"zip_state\") == zs).select(\"review_count\")\n",
    "    q_vals = counts.approxQuantile(\"review_count\", quantiles, 0.01)  # relative error 1%\n",
    "    results.append((zs, *q_vals))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_quantiles = spark.createDataFrame(results, \n",
    "                                     schema=[\"zip_state\", \"q10\", \"q25\", \"median\", \"q75\", \"q90\"])\n",
    "\n",
    "# 4) Join average and quantiles\n",
    "df_stats = df_avg_reviews.join(df_quantiles, on=\"zip_state\")\n",
    "df_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506fbf2-ec0c-461e-b92f-cd57cb247fe7",
   "metadata": {},
   "source": [
    "#### sample 5% reviews for each restaurant to predict their sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c8f528e-887b-4c22-8572-35382cc077e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original rows: 28263274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled rows: 1317433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|   sampled_reviews|\n",
      "+-------+------------------+\n",
      "|  count|            170836|\n",
      "|   mean| 7.711682549345571|\n",
      "| stddev|3.9561902133606757|\n",
      "|    min|                 1|\n",
      "|    max|                15|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===============================>                       (25 + 18) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+\n",
      "|             gmap_id|zip_state|sampled_reviews|\n",
      "+--------------------+---------+---------------+\n",
      "|0x8082d877583bdf9...|       CA|             15|\n",
      "|0x808557676646925...|       CA|             15|\n",
      "|0x54d1551005398b4...|       CA|             15|\n",
      "|0x54d067c9582c429...|       CA|             15|\n",
      "|0x54d066375c41790...|       CA|             15|\n",
      "|0x54cde6e084b0770...|       CA|             15|\n",
      "|0x80847610987d907...|       CA|             15|\n",
      "|0x54d2ece3cc6228a...|       CA|             15|\n",
      "|0x8084acb6958cbe7...|       CA|             15|\n",
      "|0x80804aaf41c5902...|       CA|             15|\n",
      "+--------------------+---------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# PARAMETERS\n",
    "min_per_store = 5\n",
    "max_per_store = 15\n",
    "sample_pct = 0.05  # 5%\n",
    "seed = 42\n",
    "\n",
    "# 1) Drop empty reviews\n",
    "df_clean = df.filter(\n",
    "    F.col(\"text\").isNotNull() & (F.length(F.col(\"text\")) > 0)\n",
    ")\n",
    "\n",
    "# 2) Count total reviews per store\n",
    "df_counts = df_clean.groupBy(\"gmap_id\").agg(F.count(\"*\").alias(\"total_reviews\"))\n",
    "\n",
    "# 3) Compute sample_size = min(max, max(min, round(pct * total)))\n",
    "df_counts = df_counts.withColumn(\n",
    "    \"sample_size\",\n",
    "    F.when(\n",
    "        F.col(\"total_reviews\") < F.lit(min_per_store),\n",
    "        F.col(\"total_reviews\")\n",
    "    ).otherwise(\n",
    "        F.least(\n",
    "            F.lit(max_per_store),\n",
    "            F.greatest(\n",
    "                F.lit(min_per_store),\n",
    "                F.round(F.col(\"total_reviews\") * F.lit(sample_pct))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) Join back sample_size into main DF\n",
    "df_joined = df_clean.join(df_counts, on=\"gmap_id\", how=\"inner\")\n",
    "\n",
    "# 5) Random rank within each store\n",
    "w = Window.partitionBy(\"gmap_id\").orderBy(F.rand(seed))\n",
    "df_ranked = df_joined.withColumn(\"rn\", F.row_number().over(w))\n",
    "\n",
    "# 6) Keep only sampled rows\n",
    "df_sampled = df_ranked.filter(F.col(\"rn\") <= F.col(\"sample_size\")) \\\n",
    "                      .drop(\"rn\")\n",
    "\n",
    "# 7) Count sampled rows per store\n",
    "df_sampled_counts = df_sampled.groupBy(\"gmap_id\", \"zip_state\") \\\n",
    "                              .agg(F.count(\"*\").alias(\"sampled_reviews\"))\n",
    "\n",
    "# 8) Print results\n",
    "print(\"Total original rows:\", df_clean.count())\n",
    "print(\"Total sampled rows:\", df_sampled.count())\n",
    "\n",
    "df_sampled_counts.describe(\"sampled_reviews\").show()\n",
    "df_sampled_counts.orderBy(F.desc(\"sampled_reviews\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a688481-e39f-47b3-919c-0792cdfefe98",
   "metadata": {},
   "source": [
    "#### Load pretain best model and transform sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38152007-8dc9-4cac-80b1-bb283015a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 17:33:11.103302: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Load the pipeline back\n",
    "embedding_model = PipelineModel.load(\"gs://msca-bdp-student-gcs/Group_5_final_project/embedding_pipeline_model\")\n",
    "\n",
    "# Transform any DataFrame without rerunning fit()\n",
    "df_emb = embedding_model.transform(df_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b871b365-0ede-46ac-88da-70f0c00d861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LinearRegressionModel successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "best_model_path = \"gs://msca-bdp-student-gcs/Group_5_final_project/cv_best_model\"\n",
    "\n",
    "cv_model = LinearRegressionModel.load(best_model_path)\n",
    "\n",
    "print(\"Loaded LinearRegressionModel successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1e0f58-60cd-43a8-96de-1b8836a4a983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Transforming sampled data → embeddings ===\n",
      "Embedding columns: ['gmap_id', 'cust_name', 'sentiment_score', 'resp', 'text', 'time', 'user_id', 'address', 'avg_rating', 'category', 'description', 'hours', 'latitude', 'longitude', 'store_name', 'num_of_reviews', 'price', 'relative_results', 'state', 'MISC_Accessibility', 'MISC_Activities', 'MISC_Amenities', 'MISC_Atmosphere', 'MISC_Crowd', 'MISC_Dining_options', 'MISC_From_the_business', 'MISC_Getting_here', 'MISC_Health_and_safety', 'MISC_Highlights', 'MISC_Lodging_options', 'MISC_Offerings', 'MISC_Payments', 'MISC_Planning', 'MISC_Popular_for', 'MISC_Recycling', 'MISC_Service_options', 'zip', 'zip_city', 'zip_state', 'zip_county', 'irs_estimated_population', 'total_reviews', 'sample_size', 'finished_sentence_embeddings', 'features']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Transforming sampled data → embeddings ===\")\n",
    "df_emb = embedding_model.transform(df_sampled).cache()\n",
    "\n",
    "print(\"Embedding columns:\", df_emb.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197d75f-fc34-42b6-a928-f345e36845e1",
   "metadata": {},
   "source": [
    "#### predict and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cfb2c01-1f83-4f17-828e-cb0a93a56c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running predictions on embeddings ===\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"=== Running predictions on embeddings ===\")\n",
    "\n",
    "df_pred = (\n",
    "    cv_model\n",
    "    .transform(df_emb)\n",
    "    .select(\n",
    "        \"gmap_id\",\n",
    "        \"text\",\n",
    "        \"sentiment_score\",\n",
    "        \"prediction\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c9a2e-7e9d-40d4-b4c7-34d896149cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predictions saved to: gs://msca-bdp-student-gcs/Group_5_final_project/full_sentiment_predictions\n"
     ]
    }
   ],
   "source": [
    "output_path = \"gs://msca-bdp-student-gcs/Group_5_final_project/full_sentiment_predictions\"\n",
    "\n",
    "(\n",
    "    df_pred\n",
    "    .repartition(32)   # write efficiently\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(output_path)\n",
    ")\n",
    "\n",
    "print(f\" Predictions saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da76f03-39d4-451f-9c38-8e6c6101221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_predictions = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/full_sentiment_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c9e81-470e-4f17-913d-a3dc2211b0a7",
   "metadata": {},
   "source": [
    "#### remove duplicate prediction row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d661d4ec-e4d9-4f27-9778-2deeb929cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 173:>            (10 + 40) / 200][Stage 183:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2634866\n",
      "Distinct rows: 1316977\n",
      "Duplicates: 1317889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 173:========>                                            (32 + 40) / 200]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "total = all_predictions.count()\n",
    "distinct_rows = all_predictions.distinct().count()\n",
    "\n",
    "print(\"Total rows:\", total)\n",
    "print(\"Distinct rows:\", distinct_rows)\n",
    "print(\"Duplicates:\", total - distinct_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b35dc272-e5c4-48b3-9eee-150b5ca61c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = all_predictions.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c8ff0cd-d140-4eef-8d73-6ca132c7f5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1316977"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2724e10e-0b05-4dc3-932c-3a1a6fb11698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+------------------+\n",
      "|             gmap_id|                text|sentiment_score|        prediction|\n",
      "+--------------------+--------------------+---------------+------------------+\n",
      "|0x80dcd72bf11458a...|This was my first...|              5| 3.487860098845287|\n",
      "|0x80c2c6488ea39f6...|The dumplings wer...|              4| 3.540126635004725|\n",
      "|0x89c29367ccb68d6...|Food was ok. Plac...|              3|3.1001709675100813|\n",
      "|0x880fd34d27973c7...|Great food.  I li...|              5| 4.604028590010108|\n",
      "|0x80c2c35e939628a...|Was visiting Pasa...|              5| 4.227837911843918|\n",
      "+--------------------+--------------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2c7f8-054d-447f-a24f-68322edbba1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b8892eb-47a8-49c2-b159-da8a371cc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev, count\n",
    "\n",
    "sentiment_by_store = all_predictions.groupBy(\"gmap_id\") \\\n",
    "    .agg(\n",
    "        avg(\"prediction\").alias(\"avg_predicted_sentiment\"),\n",
    "        stddev(\"prediction\").alias(\"sentiment_std\"),\n",
    "        count(\"*\").alias(\"review_count\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed57f5f-3709-4a05-b2a0-8da3d3ef950d",
   "metadata": {},
   "source": [
    "#### check missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1de9fd00-2439-4e0b-b0a4-defe903e10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+------------------+------------+\n",
      "|             gmap_id|avg_predicted_sentiment|     sentiment_std|review_count|\n",
      "+--------------------+-----------------------+------------------+------------+\n",
      "|0x87e2305a8b38078...|      4.220439027958734|0.6578729372473379|          15|\n",
      "|0x80c2c7b9a5c0000...|     3.8052797704237427|1.1580116502242945|           5|\n",
      "|0x80c2b789b40c1a8...|     3.9521378887751766|0.8956821516712776|          15|\n",
      "+--------------------+-----------------------+------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:====>         (71 + 40) / 200][Stage 88:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "sentiment_by_store.show(3)\n",
    "\n",
    "print(sentiment_by_store.count())\n",
    "\n",
    "# Count rows where all columns are non-null\n",
    "non_null_count = sentiment_by_store.filter(\n",
    "    col(\"avg_predicted_sentiment\").isNotNull() &\n",
    "    col(\"sentiment_std\").isNotNull() &\n",
    "    col(\"review_count\").isNotNull()\n",
    ").count()\n",
    "\n",
    "print(non_null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f89608-8f9d-4fee-bae2-42a748587c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=====>        (84 + 40) / 200][Stage 98:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "non_null_count_2 = sentiment_by_store.filter(\n",
    "    col(\"avg_predicted_sentiment\").isNotNull() &\n",
    "    col(\"review_count\").isNotNull()\n",
    ").count()\n",
    "\n",
    "print(non_null_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57ca9504-451f-47e5-a30e-ba196e78ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:======>       (93 + 40) / 200][Stage 104:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-----------------------+-------------+------------+\n",
      "|gmap_id                              |avg_predicted_sentiment|sentiment_std|review_count|\n",
      "+-------------------------------------+-----------------------+-------------+------------+\n",
      "|0x89c2592cb27cc925:0xb15bb9dac89770a3|4.635692752843019      |null         |1           |\n",
      "|0x80c2d118e3764f8d:0xb4e3b3bdd98858e0|4.05973130097594       |null         |1           |\n",
      "|0x8084487d4e09f3bb:0x39c89704eaf48ed5|4.0732281792893374     |null         |1           |\n",
      "+-------------------------------------+-----------------------+-------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sentiment_by_store.filter(\n",
    "    col(\"avg_predicted_sentiment\").isNull() |\n",
    "    col(\"sentiment_std\").isNull() |\n",
    "    col(\"review_count\").isNull()\n",
    ").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96fa11-aab2-41c7-a961-dc8580196f7c",
   "metadata": {},
   "source": [
    "#### Filter out restaurant with null sentiment_std (only 1 review_count) sentiment_by_store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "821e3c76-74d8-494e-96d6-79a0e0cb6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:======>      (100 + 40) / 200][Stage 114:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sentiment_by_store = sentiment_by_store.filter(\n",
    "    (col(\"avg_predicted_sentiment\").isNotNull()) &\n",
    "    (col(\"sentiment_std\").isNotNull()) &\n",
    "    (col(\"review_count\").isNotNull())\n",
    ")\n",
    "\n",
    "print(sentiment_by_store.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "207807ab-aeb8-47dd-a64f-ccaeeb325d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "# 1. business closure flag\n",
    "meta_df = meta_df.withColumn(\n",
    "    \"permanent_closed\",\n",
    "    when(col(\"state\").like(\"%Permanently closed%\"), 1).otherwise(0)\n",
    ")\n",
    "# 2. price mapping\n",
    "meta_df = meta_df.withColumn(\n",
    "    \"price_numeric\",\n",
    "    when(col(\"price\").isin(\"$\", \"₩\"), 1)\n",
    "    .when(col(\"price\").isin(\"$$\", \"₩₩\"), 2)\n",
    "    .when(col(\"price\").isin(\"$$$\", \"₩₩₩\"), 3)\n",
    "    .when(col(\"price\").isin(\"$$$$\", \"₩₩₩₩\"), 4)\n",
    "    .otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98675dc0-b709-4c7e-bfee-48d5c9e1f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=======>     (113 + 39) / 200][Stage 121:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+----------------------------------+---------------------------------------------------------------------------------------------------------------+-----------------------+-------------------+--------------+----------+\n",
      "|gmap_id                              |store_name                        |category                                                                                                       |avg_predicted_sentiment|sentiment_std      |num_of_reviews|avg_rating|\n",
      "+-------------------------------------+----------------------------------+---------------------------------------------------------------------------------------------------------------+-----------------------+-------------------+--------------+----------+\n",
      "|0x80c2bb365c769949:0x740be3648594442b|Big Jo's                          |[Takeout Restaurant, Cheesesteak restaurant, Restaurant, Sandwich shop]                                        |4.421576558575243      |0.7593615402001646 |585           |4.5       |\n",
      "|0x80dce9000e38b455:0x5209e315e4b9f34b|El Paraiso                        |[Salvadoran restaurant, Latin American restaurant]                                                             |4.161164494663761      |0.6619959609420497 |435           |4.3       |\n",
      "|0x89d77ccec0e80fe1:0x7dd7e39681274ca8|Oswego Sub Shop                   |[Fast food restaurant, American restaurant, Delivery Restaurant, Takeout Restaurant, Restaurant, Sandwich shop]|4.198335964841235      |1.0915555297346207 |668           |4.3       |\n",
      "|0x80c2bf2143ff306b:0x20395a8a773d1d3b|25 Degrees                        |[Hamburger restaurant, Bar & grill, Restaurant]                                                                |4.067756630048392      |1.0411120821037407 |742           |4.2       |\n",
      "|0x880fc952fee98447:0x740dac9a4743e104|Spice & Fire Grill مطعم بهار و نار|[Restaurant, Middle Eastern restaurant]                                                                        |4.559772676358964      |0.37435257430528374|226           |4.6       |\n",
      "+-------------------------------------+----------------------------------+---------------------------------------------------------------------------------------------------------------+-----------------------+-------------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming meta_df has column 'gmap_id'\n",
    "business_df = meta_df.join(\n",
    "    sentiment_by_store,\n",
    "    on=\"gmap_id\",\n",
    "    how=\"inner\"  \n",
    ")\n",
    "\n",
    "business_df.select(\n",
    "    \"gmap_id\", \"store_name\", \"category\", \"avg_predicted_sentiment\",\n",
    "    \"sentiment_std\", \"num_of_reviews\", \"avg_rating\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a497de-12ab-4e93-bc09-5f8cf571cc1c",
   "metadata": {},
   "source": [
    "#### check correlation with avg_rating and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f45cca3-b3cc-44ce-86e7-df06bbf68866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=======>     (121 + 39) / 200][Stage 129:===========>     (4 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.6312120921010704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=======================================>             (149 + 40) / 200]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# df = your dataframe\n",
    "\n",
    "# Keep only needed columns\n",
    "corr_df = business_df.select(\n",
    "    \"avg_rating\",\n",
    "    \"avg_predicted_sentiment\"\n",
    ").dropna()\n",
    "\n",
    "# --- Simple correlation (Pearson) ---\n",
    "pearson_corr = corr_df.stat.corr(\"avg_rating\", \"avg_predicted_sentiment\")\n",
    "print(\"Pearson correlation:\", pearson_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26c70d6f-6f2e-47a6-99f6-89d5b4607519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:===========================================>         (164 + 36) / 200]\r"
     ]
    }
   ],
   "source": [
    "business_df.write.mode(\"overwrite\").parquet(\"gs://msca-bdp-student-gcs/Group_5_final_project/store_df/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949bdf7-4b9a-477c-9ad2-feb7e898c506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
